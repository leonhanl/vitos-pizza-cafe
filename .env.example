# ============================================================================
# Vito's Pizza Cafe - Environment Configuration
# ============================================================================
#
# SETUP INSTRUCTIONS:
# 1. Copy this file and rename it to .env
# 2. Replace all placeholder values (your_*_api_key_here) with actual API keys
# 3. Uncomment the configuration block for your chosen LLM provider
# 4. Ensure the .env file is never committed to version control
#
# ============================================================================


# ============================================================================
# LLM PROVIDER CONFIGURATION
# ============================================================================
# The application supports multiple LLM providers via OpenAI-compatible APIs.
# Uncomment ONE configuration block below based on your provider.
# ----------------------------------------------------------------------------

# Option 1: OpenAI (Default)
# ----------------------------------------------------------------------------
# Use official OpenAI API. No need to specify OPENAI_BASE_URL.
OPENAI_API_KEY=your_openai_api_key_here
LLM_MODEL=gpt-5-mini
# Available models: gpt-5, gpt-5-mini, gpt-5-nano

# Option 2: DeepSeek
# ----------------------------------------------------------------------------
# Use DeepSeek's OpenAI-compatible API endpoint.
# OPENAI_API_KEY=your_deepseek_api_key_here
# OPENAI_BASE_URL=https://api.deepseek.com/v1
# LLM_MODEL=deepseek-chat

# Option 3: LiteLLM Proxy
# ----------------------------------------------------------------------------
# Route requests through LiteLLM proxy for unified model access.
# See litellm/ directory for Docker Compose setup.
# OPENAI_API_KEY=your_litellm_api_key_here
# OPENAI_BASE_URL=http://localhost:4000
# LLM_MODEL=deepseek/deepseek-chat

# Option 4: OpenRouter
# ----------------------------------------------------------------------------
# Access multiple models through OpenRouter's unified API.
# OPENAI_API_KEY=your_openrouter_api_key_here
# OPENAI_BASE_URL=https://openrouter.ai/api/v1
# LLM_MODEL=openai/gpt-5-mini


# ============================================================================
# EMBEDDING MODEL CONFIGURATION
# ============================================================================
# Used for RAG (Retrieval-Augmented Generation) vector embeddings.
# Currently supports OpenAI and OpenRouter providers.
# ----------------------------------------------------------------------------

# Default: Use OpenAI embeddings with the same API key as LLM
# If OPENAI_EMBEDDING_API_KEY is not set, it defaults to OPENAI_API_KEY
EMBEDDING_MODEL=text-embedding-3-small
# Available OpenAI models: text-embedding-3-small, text-embedding-3-large, text-embedding-ada-002

# Optional: Use separate API credentials for embeddings
# ----------------------------------------------------------------------------
# Uncomment these lines to use different credentials or provider for embeddings.
# Example: Use DeepSeek for LLM but OpenAI for embeddings
# OPENAI_EMBEDDING_API_KEY=your_openai_api_key_here
# OPENAI_EMBEDDING_BASE_URL defaults to https://api.openai.com/v1 if not specified

# Optional: Use OpenRouter for embeddings
# ----------------------------------------------------------------------------
# OPENAI_EMBEDDING_API_KEY=your_openrouter_api_key_here
# OPENAI_EMBEDDING_BASE_URL=https://openrouter.ai/api/v1
# EMBEDDING_MODEL=openai/text-embedding-3-small


# ============================================================================
# PALO ALTO NETWORKS AI RUNTIME SECURITY (AIRS)
# ============================================================================
# Configuration for AI Runtime Security (X-Pan) integration.
# Provides input/output safety checks and security monitoring.
# ----------------------------------------------------------------------------
# Enable AIRS scanning (set to true to activate input/output security checks)
AIRS_ENABLED=false

X_PAN_TOKEN=your_xpan_token_here
X_PAN_AI_MODEL=gpt-5-mini
X_PAN_APP_NAME='Vitos Pizza Cafe'
X_PAN_APP_USER='Vitos-Admin'
X_PAN_INPUT_CHECK_PROFILE_NAME='Demo-Profile-for-Input'
X_PAN_OUTPUT_CHECK_PROFILE_NAME='Demo-Profile-for-Output'

# AIRS Streaming Configuration
# ----------------------------------------------------------------------------
# Scan accumulated content every N chunks during streaming responses
AIRS_STREAM_SCAN_CHUNK_INTERVAL=50


# ============================================================================
# OPTIONAL INTEGRATIONS
# ============================================================================

# MCP Tools Configuration
# ============================================================================
# IMPORTANT: Choose ONE integration approach (mutually exclusive):
# - Direct Connection: Enable AMAP-SSE or AMAP-STDIO (configure here)
# - Proxy Mode: Enable PAN MCP Relay (all MCP servers go in pan-mcp-relay/mcp-relay.yaml)

# AMAP Location Services - Direct Connection
# ----------------------------------------------------------------------------
# The application demonstrates two transport types for direct AMAP integration:
# 1. SSE (Server-Sent Events) - HTTP-based streaming connection
# 2. STDIO (Standard Input/Output) - Local subprocess via uvx
# NOTE: Disable all direct connections when using PAN MCP Relay (set all to false)
AMAP_API_KEY=your_amap_api_key_here

# Enable AMAP-SSE transport (set to true)
AMAP_SSE_ENABLED=false

# Enable AMAP-STDIO transport (set to true, requires uvx installed)
AMAP_STDIO_ENABLED=false

# Python Code Execution (code-sandbox-mcp) - Direct Connection
# ----------------------------------------------------------------------------
# Provides isolated Docker container environment for safe Python code execution.
# Use cases: Data analysis, calculations, code validation, testing snippets.
# Requirements: Docker must be installed and running
# Installation:
#   1. Install binary: curl -fsSL https://raw.githubusercontent.com/Automata-Labs-team/code-sandbox-mcp/main/install.sh | bash
#      Default install location: ~/.local/share/code-sandbox-mcp/code-sandbox-mcp
#      Windows: irm https://raw.githubusercontent.com/Automata-Labs-team/code-sandbox-mcp/main/install.ps1 | iex
#   2. Pull Docker image (required): docker pull python:3.12-slim-bookworm
# NOTE: Disable when using PAN MCP Relay (set to false)

# Path to code-sandbox-mcp binary (required if enabled)
CODE_SANDBOX_MCP_PATH=/Users/yourusername/.local/share/code-sandbox-mcp/code-sandbox-mcp

# Enable Python code execution (set to true)
PYTHON_EXEC_MCP_ENABLED=false

# PAN MCP Relay - Centralized Security Proxy
# ----------------------------------------------------------------------------
# Security-enhanced MCP relay that acts as a centralized gateway for all MCP tools.
# When enabled, ALL MCP servers must be configured in pan-mcp-relay/mcp-relay.yaml
# (not here in .env). The relay provides AIRS threat scanning for tool interactions.
#
# Setup: Start relay with: cd pan-mcp-relay && ./start_pan_mcp_relay.sh
# NOTE: When enabled, disable all direct MCP connections above (set all to false)
PAN_MCP_RELAY_ENABLED=false
PAN_MCP_RELAY_URL=http://127.0.0.1:8800/mcp/

# LangSmith Tracing and Monitoring
# ----------------------------------------------------------------------------
# Enable LangSmith for conversation flow debugging and monitoring.
LANGSMITH_TRACING=false
LANGSMITH_API_KEY=your_langsmith_api_key_here
LANGSMITH_ENDPOINT=https://api.smith.langchain.com
LANGSMITH_PROJECT=vitos-pizza-cafe


# ============================================================================
# APPLICATION SETTINGS
# ============================================================================

# Logging Configuration
# ----------------------------------------------------------------------------
# Available levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO